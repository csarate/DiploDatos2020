{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento Infoleg y Datasets Anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import docx2txt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import pprint as pprint\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# %load_ext lab_black\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc, [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Se verfica entorno de ejecución\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
    "else:\n",
    "    BASE_DIR = \"./Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Definición de directorios y nombres para train y test data\n",
    "train_data_dir = \"{}\".format(os.sep).join([BASE_DIR, \"Infoleg/\"])\n",
    "test_data_dir = \"{}\".format(os.sep).join([BASE_DIR, \"Infoleg_test/\"])\n",
    "train_data = train_data_dir + \"Infoleg_train.cor\"\n",
    "test_data = test_data_dir + \"Infoleg_test.cor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Data/Infoleg/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'./Data/Infoleg_test/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'./Data/Infoleg/Infoleg_train.cor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'./Data/Infoleg_test/Infoleg_test.cor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data_dir)\n",
    "display(test_data_dir)\n",
    "display(train_data)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train = next(os.walk(train_data_dir))\n",
    "except StopIteration:\n",
    "    pass  # Some error handling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test = next(os.walk(test_data_dir))\n",
    "except StopIteration:\n",
    "    pass  # Some error handling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Limpieza del texto con la opción remover stopwords\n",
    "\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "\n",
    "    # Conversión de palabras a minúsculas y separación\n",
    "    words = review.lower().split()\n",
    "\n",
    "    # Opcionalmente se remueven stop words (true por default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"spanish\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    review_text = \" \".join(words)\n",
    "\n",
    "    # Clean the text\n",
    "    review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'s\", \" 's \", review_text)\n",
    "    review_text = re.sub(r\"\\'ve\", \" 've \", review_text)\n",
    "    review_text = re.sub(r\"n\\'t\", \" 't \", review_text)\n",
    "    review_text = re.sub(r\"\\'re\", \" 're \", review_text)\n",
    "    review_text = re.sub(r\"\\'d\", \" 'd \", review_text)\n",
    "    review_text = re.sub(r\"\\'ll\", \" 'll \", review_text)\n",
    "    review_text = re.sub(r\",\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\.\", \" \", review_text)\n",
    "    review_text = re.sub(r\"!\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\(\", \" ( \", review_text)\n",
    "    review_text = re.sub(r\"\\)\", \" ) \", review_text)\n",
    "    review_text = re.sub(r\"\\?\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
    "\n",
    "    words = review_text.split()\n",
    "\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    review_text = \" \".join(stemmed_words)\n",
    "\n",
    "    # Return a list of words\n",
    "    return review_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion del Corpus de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicion de una Funcion para Leer y Preprocesar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_text(filepath, file):\n",
    "    docx_file = file\n",
    "    text = docx2txt.process(docx_file)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Búsqueda de archivos .Doc y renombrado a .Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc_files = []\n",
    "path_dir = os.getcwd()\n",
    "for root, subdirs, files in os.walk(path_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".doc\"):\n",
    "            doc_files.append(((root, subdirs, file)))\n",
    "        elif file.endswith(\".docx\"):\n",
    "            doc_files.append(((root, subdirs, file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenación de archivos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "docLabels = []\n",
    "row_list = []\n",
    "i = 0\n",
    "\n",
    "for folder, subfolders, filenames in os.walk(train_data_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".txt\"):\n",
    "            d = {folder}\n",
    "            with open(folder + file) as f:\n",
    "                if f.read():\n",
    "                    f.seek(0)\n",
    "                    d = f.read()\n",
    "                    docLabels.append(folder + file)\n",
    "                    row_list.append(review_to_wordlist(d))\n",
    "                    i = i + 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos txt de entrenamiento leidos 8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u\"Archivos txt de entrenamiento leidos %s \\n\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barrido de Archivos .Doc para Conversion y Grabacion como Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "txt_file = \"\"\n",
    "i = 0\n",
    "\n",
    "for path, subdir, file in doc_files:\n",
    "    doc_file = str(path) + \"/\" + str(file)\n",
    "\n",
    "    if file.endswith(\".doc\"):\n",
    "        docx_file = doc_file + \"x\"\n",
    "        if not os.path.exists(docx_file):\n",
    "            mycmd = \"antiword \" + \"'\" + doc_file + \"'\" + \" > \" + \"'\"\n",
    "            +docx_file + \"'\"\n",
    "            os.system(mycmd)\n",
    "            os.remove(\"'\" + doc_file + \"'\")  # it was just to read,so deleting\n",
    "    elif file.endswith(\".docx\"):\n",
    "        docx_file = doc_file\n",
    "\n",
    "    txt_file = get_doc_text(str(path), docx_file)\n",
    "    docLabels.append(str(path) + \"/\" + txt_file)\n",
    "    row_list.append(review_to_wordlist(txt_file))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos docx leidos y procesados a txt 153:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u\"Archivos docx leidos y procesados a txt %s:\\n\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# iterator returned over all documents\n",
    "it = LabeledLineSentence(row_list, docLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ente nacional regul de la electr resoluci n en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>administraci n federal de ingres p blic impues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>defens del consumidor ley n 24 240 norm de pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>direccion general de aduan resoluci n 63 2008 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indemniz desaparicion forz de person ley 25 98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ministeri del interior bomber voluntari resolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ente nacional regul del gas tarif resoluci n 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ministeri de justici segur y derech human decr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sal electoral y de comp originari tribunal sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sal electoral y de comp originari tribunal sup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  ente nacional regul de la electr resoluci n en...\n",
       "1  administraci n federal de ingres p blic impues...\n",
       "2  defens del consumidor ley n 24 240 norm de pro...\n",
       "3  direccion general de aduan resoluci n 63 2008 ...\n",
       "4  indemniz desaparicion forz de person ley 25 98...\n",
       "5  ministeri del interior bomber voluntari resolu...\n",
       "6  ente nacional regul del gas tarif resoluci n 3...\n",
       "7  ministeri de justici segur y derech human decr...\n",
       "8  sal electoral y de comp originari tribunal sup...\n",
       "9  sal electoral y de comp originari tribunal sup..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(row_list)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generación del corpus de entrenamiento\n",
    "df_train.to_csv(train_data, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenación de archivos de test\n",
    "testLabels = []\n",
    "row_list = []\n",
    "i = 0\n",
    "\n",
    "for folder, subfolders, filenames in os.walk(test_data_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".txt\"):\n",
    "            d = {folder}\n",
    "            with open(folder + file) as f:\n",
    "                if f.read():\n",
    "                    f.seek(0)\n",
    "                    d = f.read()\n",
    "                    testLabels.append(folder + file)\n",
    "                    row_list.append(review_to_wordlist(d))\n",
    "                    i = i + 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos txt de test leidos 3:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u\"Archivos txt de test leidos %s:\\n\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>secret a de agricultur ganad a pesc y aliment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>administracion federal de ingres public direcc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>administracion federal de ingres public direcc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  secret a de agricultur ganad a pesc y aliment ...\n",
       "1  administracion federal de ingres public direcc...\n",
       "2  administracion federal de ingres public direcc..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(row_list)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generación del corpus de test\n",
    "df_test.to_csv(test_data_dir + \"Infoleg_test.cor\", sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicion de una Funcion para Leer y Preprocesar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # Entrenamiento de datos y tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(\n",
    "                    gensim.utils.simple_preprocess(line), [i]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damos un vistazo al corpus de entrenamiento"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint.pprint(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora al corpus de test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint.pprint(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancia de un Objeto Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=128, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de un Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Definición de diferentes alternativas de modelo\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW\n",
    "    Doc2Vec(\n",
    "        dm=0,\n",
    "        dbow_words=1,\n",
    "        vector_size=200,\n",
    "        window=8,\n",
    "        min_count=19,\n",
    "        epochs=10,\n",
    "        workers=cores,\n",
    "    ),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(\n",
    "        dm=1,\n",
    "        dm_mean=1,\n",
    "        vector_size=200,\n",
    "        window=8,\n",
    "        min_count=19,\n",
    "        epochs=10,\n",
    "        workers=cores,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc19,s0.001,t4)\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "models[0].build_vocab(train_corpus)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momento de Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 427 ms, total: 2min 36s\n",
      "Wall time: 53.4 s\n",
      "CPU times: user 29.4 s, sys: 192 ms, total: 29.5 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    %time model.train(train_corpus, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infiriendo un Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03543337, -0.02179321, -0.0186638 ,  0.02324385, -0.01308073,\n",
       "        0.0104502 ,  0.01946487,  0.02467658,  0.00737994,  0.01650654,\n",
       "        0.03434812,  0.00699867, -0.00207358, -0.02242089,  0.0146283 ,\n",
       "        0.01501486,  0.02557651, -0.0006762 ,  0.00915986,  0.06618509,\n",
       "       -0.0242402 , -0.00293844,  0.00793903,  0.01735396, -0.01516493,\n",
       "        0.00178138,  0.01627547,  0.02827312,  0.01210817,  0.0386159 ,\n",
       "        0.02465968, -0.01833941, -0.01266908, -0.03641306, -0.00432767,\n",
       "       -0.03514048, -0.00524915,  0.0297388 , -0.00699908,  0.00695747,\n",
       "        0.00651056, -0.02961001,  0.05350425, -0.0047966 ,  0.05040609,\n",
       "        0.02263964,  0.04255007, -0.02451929, -0.00087611, -0.00566807,\n",
       "        0.00282642,  0.05884235,  0.01917174,  0.00234831,  0.04085301,\n",
       "       -0.04272524,  0.00304498,  0.01290464,  0.02652242, -0.00544464,\n",
       "       -0.02732924,  0.03729615, -0.00110397, -0.00699362, -0.02283825,\n",
       "        0.02166421,  0.00213674, -0.00628853,  0.01079531, -0.02954101,\n",
       "       -0.01845518, -0.06967127, -0.01080515, -0.01435294, -0.00792418,\n",
       "       -0.01428687,  0.02223295,  0.00297635, -0.00848168,  0.03689951,\n",
       "        0.04430898,  0.06148484, -0.02734691, -0.00320769,  0.01509317,\n",
       "        0.04294666,  0.03384944,  0.01508023,  0.00322015, -0.01810081,\n",
       "       -0.00590096,  0.04494928,  0.0189895 ,  0.01096422,  0.02627121,\n",
       "        0.01624159, -0.02796598,  0.04477835,  0.02011338,  0.00720123,\n",
       "       -0.03549813, -0.01573027,  0.0273615 ,  0.04505707,  0.05402576,\n",
       "        0.00420072,  0.01531666,  0.01650379, -0.02813117, -0.01503137,\n",
       "        0.02198874,  0.0142666 ,  0.01542351, -0.03510112, -0.05188898,\n",
       "       -0.07803196, -0.03207371, -0.01955296,  0.00050289,  0.01824377,\n",
       "       -0.03526201, -0.02142293, -0.05146253,  0.02149433, -0.06074703,\n",
       "        0.02306544, -0.02633608,  0.01600245, -0.01071916, -0.04637476,\n",
       "       -0.00490157,  0.04378555, -0.00347257,  0.02841086,  0.04238335,\n",
       "        0.01363264, -0.00024079,  0.02795732, -0.00074848, -0.00438277,\n",
       "        0.01183843,  0.01105481,  0.01184548, -0.00758423, -0.02691103,\n",
       "       -0.02197817,  0.03310759,  0.00380455,  0.00594311,  0.03115225,\n",
       "       -0.02191091, -0.04453721,  0.0036804 , -0.00340719, -0.03878275,\n",
       "        0.03624243,  0.01099525,  0.0229016 ,  0.00199342,  0.02799155,\n",
       "       -0.00138335, -0.02007883, -0.00533208,  0.05924188,  0.00579874,\n",
       "        0.02497473, -0.02162671,  0.0200703 ,  0.02609314, -0.07488388,\n",
       "       -0.06785863, -0.01479853, -0.02698742,  0.06031273,  0.02379957,\n",
       "       -0.01461495,  0.05349847,  0.00996625, -0.01643927, -0.00568579,\n",
       "        0.02970131,  0.04163876, -0.00531479,  0.01110792, -0.02535948,\n",
       "        0.00128328,  0.01426201,  0.00325961,  0.01419605,  0.05072262,\n",
       "        0.02218011,  0.01250447, -0.00662057,  0.00788567, -0.02380656,\n",
       "        0.01999045, -0.00799787,  0.02934437,  0.02579496, -0.01363106],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(\n",
    "    [\"ejecucion\", \"nacional\", \"acuerdo\", \"procesos\", \"aplicacion\", \"programa\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cuenta como cada documento rankea con respecto al corpus de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({123: 1,\n",
       "         0: 131,\n",
       "         5: 1,\n",
       "         1: 9,\n",
       "         2: 6,\n",
       "         14: 1,\n",
       "         4: 2,\n",
       "         3: 2,\n",
       "         9: 3,\n",
       "         6: 3,\n",
       "         11: 1,\n",
       "         13: 2})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los resultados pueden variar debido a la semilla random y a un corpus muy pequeño\n",
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCS SIMILARES POR MODELO Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t4):\n",
      "\n",
      "MAS SIMILAR  ./Data/Infoleg/124898.txt\n",
      "SEGUNDO  ./Data/Infoleg/113710.txt\n",
      "TERCERO ./Data/Infoleg/638.txt\n"
     ]
    }
   ],
   "source": [
    "# print(\"Documentos ({}): «{}»\\n\".format(doc_id, \" \".join(train_corpus[doc_id].words)))\n",
    "print(u\"DOCS SIMILARES POR MODELO %s:\\n\" % model)\n",
    "for label, index in [\n",
    "    (\"MAS SIMILAR \", 0),\n",
    "    (\"SEGUNDO \", 1),\n",
    "    (\"TERCERO\", 2),\n",
    "    #    (\"LEAST\", len(sims) - 1),\n",
    "]:\n",
    "    print(label, docLabels[index])\n",
    "#        u\"%s %s: «%s»\\n\"\n",
    "#        % (label, sims[index], \" \".join(train_corpus[sims[index][0]].words))\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a similarity score approaching 1.0. However, the similarity score for the second-ranked documents should be significantly lower (assuming the documents are in fact different) and the reasoning becomes obvious when we examine the text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document comparisons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCS SIMILARES POR MODELO Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t4):\n",
      "\n",
      "MAS SIMILAR  ./Data/Infoleg/124898.txt\n",
      "SEGUNDO  ./Data/Infoleg/113710.txt\n",
      "TERCERO ./Data/Infoleg/638.txt\n"
     ]
    }
   ],
   "source": [
    "# Tomamos un documento random para testear el corpus e inferir un vector desde the modelo\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "doc_test = testLabels[doc_id]\n",
    "\n",
    "print(u\"DOCS SIMILARES POR MODELO %s:\\n\" % model)\n",
    "for label, index in [\n",
    "    (\"MAS SIMILAR \", 0),\n",
    "    (\"SEGUNDO \", 1),\n",
    "    (\"TERCERO\", 2),\n",
    "    #    (\"LEAST\", len(sims) - 1),\n",
    "]:\n",
    "    print(label, docLabels[index])\n",
    "#        u\"%s %s: «%s»\\n\"\n",
    "#        % (label, sims[index], \" \".join(train_corpus[sims[index][0]].words))\n",
    "#    )\n",
    "\n",
    "# print(\"Testeo Documento ({}): «{}»\\n\".format(doc_id, \" \".join(test_corpus[doc_id])))\n",
    "# print(\"Testeo Documento ({}): «{}»\\n\".format(doc_id, \" \".join(testLabels[doc_id]))\n",
    "\n",
    "# print(u\"DOCS SIMILARES POR MODELO %s:\\n\" % model)\n",
    "# for label, index in [(\"MAS SIMILAR\", 0), (\"SEGUNDO\", 1), (\"TERC\", 2)]:\n",
    "#    print((sims[index], docLabels[index])\n",
    "#       u\"%s %s: «%s»\\n\"\n",
    "#        %\n",
    "#        % (label, sims[index], \" \".join(train_corpus[sims[index][0]].words))\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabación Modelo Serializado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/my_doc2vec_model\n"
     ]
    }
   ],
   "source": [
    "file_name = BASE_DIR + \"/my_doc2vec_model\"\n",
    "print(file_name)\n",
    "model.save(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "output_auto_scroll": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
